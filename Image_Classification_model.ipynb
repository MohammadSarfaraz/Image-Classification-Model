{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image Classification model",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN6ZEG/vRg28oEiTw71nY7b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MohammadSarfaraz/Image-Classification-Model/blob/main/Image_Classification_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jXk8afp9Pog"
      },
      "source": [
        "# Introduction\n",
        "* Convolutional neural networks (CNN) â€“ the concept behind recent breakthroughs  and developments in deep learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJqWXO9G9Xjc"
      },
      "source": [
        "CNNs have broken the mold and ascended the throne to become the state-of-the-art computer vision technique. Among the different types of neural networks (others include recurrent neural networks (RNN), long short term memory (LSTM), artificial neural networks (ANN), etc.), CNNs are easily the most popular.\n",
        "\n",
        "There are various datasets that you can leverage for applying convolutional neural networks. Here are three popular datasets:\n",
        "\n",
        "* MNIST\n",
        "* CIFAR-10\n",
        "* ImageNet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-si_smFL3qSJ"
      },
      "source": [
        "# Using CNNs to Classify Hand-written Digits on MNIST Dataset\n",
        "###  MNIST CNN\n",
        "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/02/mnist.png)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "MNIST (Modified National Institute of Standards and Technology) is a well-known dataset used in Computer Vision that was built by Yann Le Cun et. al. It is composed of images that are handwritten digits (0-9), split into a training set of 50,000 images and a test set of 10,000 where each image is of 28 x 28 pixels in width and height."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xM5S_tiNdT5e"
      },
      "source": [
        "# keras imports for the dataset and building our neural network\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten\n",
        "from keras.utils import np_utils\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ar8pCFOA1pQW"
      },
      "source": [
        "# to calculate accuracy\n",
        "from sklearn.metrics import accuracy_score\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "li0yJlIK1xKQ",
        "outputId": "2bac3004-763c-4052-ef1c-e88aeb70dcc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# loading the dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# building the input vector from the 28x28 pixels\n",
        "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWirlaMn17lw",
        "outputId": "38a27b4f-2293-4602-c100-594797c76288",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# normalizing the data to help with the training\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "# one-hot encoding using keras' numpy-related utilities\n",
        "n_classes = 10\n",
        "print(\"Shape before one-hot encoding: \", y_train.shape)\n",
        "Y_train = np_utils.to_categorical(y_train, n_classes)\n",
        "Y_test = np_utils.to_categorical(y_test, n_classes)\n",
        "print(\"Shape after one-hot encoding: \", Y_train.shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape before one-hot encoding:  (60000,)\n",
            "Shape after one-hot encoding:  (60000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YxAzn_32O_9"
      },
      "source": [
        "# building a linear stack of layers with the sequential model\n",
        "model = Sequential()\n",
        "# convolutional layer\n",
        "model.add(Conv2D(25, kernel_size=(3,3), strides=(1,1), padding='valid', activation='relu', input_shape=(28,28,1)))\n",
        "model.add(MaxPool2D(pool_size=(1,1)))\n",
        "# flatten output of conv\n",
        "model.add(Flatten())\n",
        "# hidden layer\n",
        "model.add(Dense(100, activation='relu'))\n",
        "# output layer\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# compiling the sequential model\n",
        "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmI_vqsm2SbF",
        "outputId": "a211622f-5759-4c8e-ed09-fd1851103c2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# training the model for 10 epochs\n",
        "model.fit(X_train, Y_train, batch_size=128, epochs=10, validation_data=(X_test, Y_test))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "469/469 [==============================] - 38s 81ms/step - loss: 0.1982 - accuracy: 0.9415 - val_loss: 0.0744 - val_accuracy: 0.9771\n",
            "Epoch 2/10\n",
            "469/469 [==============================] - 38s 81ms/step - loss: 0.0614 - accuracy: 0.9821 - val_loss: 0.0594 - val_accuracy: 0.9808\n",
            "Epoch 3/10\n",
            "469/469 [==============================] - 38s 81ms/step - loss: 0.0363 - accuracy: 0.9893 - val_loss: 0.0520 - val_accuracy: 0.9825\n",
            "Epoch 4/10\n",
            "469/469 [==============================] - 38s 81ms/step - loss: 0.0230 - accuracy: 0.9933 - val_loss: 0.0519 - val_accuracy: 0.9839\n",
            "Epoch 5/10\n",
            "469/469 [==============================] - 38s 81ms/step - loss: 0.0149 - accuracy: 0.9957 - val_loss: 0.0549 - val_accuracy: 0.9834\n",
            "Epoch 6/10\n",
            "469/469 [==============================] - 38s 81ms/step - loss: 0.0103 - accuracy: 0.9968 - val_loss: 0.0562 - val_accuracy: 0.9840\n",
            "Epoch 7/10\n",
            "469/469 [==============================] - 38s 81ms/step - loss: 0.0084 - accuracy: 0.9976 - val_loss: 0.0574 - val_accuracy: 0.9845\n",
            "Epoch 8/10\n",
            "469/469 [==============================] - 38s 81ms/step - loss: 0.0050 - accuracy: 0.9988 - val_loss: 0.0551 - val_accuracy: 0.9854\n",
            "Epoch 9/10\n",
            "469/469 [==============================] - 38s 81ms/step - loss: 0.0043 - accuracy: 0.9987 - val_loss: 0.0674 - val_accuracy: 0.9827\n",
            "Epoch 10/10\n",
            "469/469 [==============================] - 38s 81ms/step - loss: 0.0051 - accuracy: 0.9985 - val_loss: 0.0619 - val_accuracy: 0.9838\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fee02b74c88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOlJaaon4Eca"
      },
      "source": [
        "# Identifying Images from the CIFAR-10 Dataset using CNNs\n",
        "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/02/1_sGochNLZ-qfesdyjadgXNw.png)\n",
        "\n",
        "The important points that distinguish this dataset from MNIST are:\n",
        "\n",
        "\n",
        "\n",
        "Images are colored in CIFAR-10 as compared to the black and white texture of MNIST\n",
        "\n",
        "* Each image is 32 x 32 colour pixel/images in 10 classes,\n",
        "* 50,000 training images and \n",
        "* 10,000 testing images.\n",
        "\n",
        "Now, these images are taken in varying lighting conditions and at different angles, and since these are colored images, you will see that there are many variations in the color itself of similar objects (for example, the color of ocean water). If you use the simple CNN architecture that we saw in the MNIST example above, you will get a low validation accuracy of around 60%.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoFaO5M55a29",
        "outputId": "7833b2bd-d347-4fb9-b8f9-e1f8b3ddd6a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from keras.datasets import cifar10\n",
        "# loading the dataset \n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zc-d3Tou5vL_",
        "outputId": "8d34b985-4fcd-43c7-c4eb-42542382e1a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# # building the input vector from the 32x32 pixels\n",
        "X_train = X_train.reshape(X_train.shape[0], 32, 32, 3)\n",
        "X_test = X_test.reshape(X_test.shape[0], 32, 32, 3)\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "# normalizing the data to help with the training\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "# one-hot encoding using keras' numpy-related utilities\n",
        "n_classes = 10\n",
        "print(\"Shape before one-hot encoding: \", y_train.shape)\n",
        "Y_train = np_utils.to_categorical(y_train, n_classes)\n",
        "Y_test = np_utils.to_categorical(y_test, n_classes)\n",
        "print(\"Shape after one-hot encoding: \", Y_train.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape before one-hot encoding:  (50000, 1)\n",
            "Shape after one-hot encoding:  (50000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBhU5ig46THh"
      },
      "source": [
        "Hereâ€™s what I had to changed in the model:\n",
        "* Increased the number of Conv2D layers to build a deeper model\n",
        "* Increased number of filters to learn more features\n",
        "* Added Dropout for regularization\n",
        "* Added more Dense layers\n",
        "\n",
        "Training and validation accuracy across epochs:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68cckQnW53Yg",
        "outputId": "fc6c5c9e-780e-4c06-d05f-f04b9b324474",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# building a linear stack of layers with the sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# convolutional layer\n",
        "model.add(Conv2D(50, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu', input_shape=(32, 32, 3)))\n",
        "\n",
        "# convolutional layer\n",
        "model.add(Conv2D(75, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(125, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# flatten output of conv\n",
        "model.add(Flatten())\n",
        "\n",
        "# hidden layer\n",
        "model.add(Dense(500, activation='relu'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(250, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "# output layer\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# compiling the sequential model\n",
        "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
        "\n",
        "# training the model for 10 epochs\n",
        "model.fit(X_train, Y_train, batch_size=128, epochs=10, validation_data=(X_test, Y_test))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "391/391 [==============================] - 557s 1s/step - loss: 1.5768 - accuracy: 0.4228 - val_loss: 1.1648 - val_accuracy: 0.5890\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 557s 1s/step - loss: 1.0880 - accuracy: 0.6146 - val_loss: 0.8928 - val_accuracy: 0.6891\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 550s 1s/step - loss: 0.9123 - accuracy: 0.6805 - val_loss: 0.8036 - val_accuracy: 0.7283\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 549s 1s/step - loss: 0.8024 - accuracy: 0.7208 - val_loss: 0.7554 - val_accuracy: 0.7389\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 551s 1s/step - loss: 0.7135 - accuracy: 0.7510 - val_loss: 0.7174 - val_accuracy: 0.7507\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 550s 1s/step - loss: 0.6493 - accuracy: 0.7731 - val_loss: 0.6872 - val_accuracy: 0.7638\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 555s 1s/step - loss: 0.5877 - accuracy: 0.7938 - val_loss: 0.7010 - val_accuracy: 0.7600\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 556s 1s/step - loss: 0.5452 - accuracy: 0.8087 - val_loss: 0.6633 - val_accuracy: 0.7744\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 554s 1s/step - loss: 0.5064 - accuracy: 0.8218 - val_loss: 0.6539 - val_accuracy: 0.7788\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 553s 1s/step - loss: 0.4678 - accuracy: 0.8356 - val_loss: 0.6482 - val_accuracy: 0.7796\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fedfed8b518>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDF8fyvZ7OPw"
      },
      "source": [
        "# Categorizing the Images of ImageNet using CNNs\n",
        "\n",
        "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/02/ImageNet-Title-Pic.jpg)\n",
        "\n",
        "ImageNet is the main database behind the ImageNet Large Scale Recognition Challenge (ILSVRC). This is like the Olympics of Computer Vision. This is the competition that made CNNs popular the first time and every year, the best research teams across industries and academia compete with their best algorithms on computer vision tasks.\n",
        "\n",
        " \n",
        "\n",
        "About the ImageNet Dataset\n",
        "The ImageNet dataset has more than 14 million images, hand-labeled across 20,000 categories.\n",
        "\n",
        "Also, unlike the MNIST and CIFAR-10 datasets that we have already discussed, the images in ImageNet are of decent resolution (224 x 224) and thatâ€™s what poses a challenge for us: 14 million images, each 224 by 224 pixels. Processing a dataset of this size requires a great amount of computing power in terms of CPU, GPU, and RAM.\n",
        "\n",
        "The downside â€“ that might be too much for an everyday laptop. So whatâ€™s the alternative solution? How can an enthusiast work with the ImageNet dataset?\n",
        "\n",
        " \n",
        "\n",
        "Thatâ€™s where Fast.aiâ€™s Imagenette dataset comes in\n",
        "Imagenette is a dataset thatâ€™s extracted from the large ImageNet collection of images. The reason behind releasing Imagenette is that researchers and students can practice on ImageNet level images without needing that much compute resources.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XO_-1eWHSHza",
        "outputId": "69ef15cf-c471-4891-b95e-97dbb59a0a7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget https://s3.amazonaws.com/fast-ai-imageclas/imagenette2.tgz"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-08 08:54:28--  https://s3.amazonaws.com/fast-ai-imageclas/imagenette2.tgz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.64.139\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.64.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1556914727 (1.4G) [application/x-tar]\n",
            "Saving to: â€˜imagenette2.tgzâ€™\n",
            "\n",
            "imagenette2.tgz     100%[===================>]   1.45G  67.9MB/s    in 26s     \n",
            "\n",
            "2020-10-08 08:54:55 (56.6 MB/s) - â€˜imagenette2.tgzâ€™ saved [1556914727/1556914727]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_y3RfLUSgnq"
      },
      "source": [
        "!tar -xf imagenette2.tgz"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svxp4h3pTHD0"
      },
      "source": [
        "2. Loading Images using ImageDataGenerator\n",
        "Keras has this useful functionality for loading large images (like we have here) without maxing out the RAM, by doing it in small batches. ImageDataGenerator in combination with fit_generator provides this functionality:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwAvejGPS8uW",
        "outputId": "8f80427e-3042-4859-f564-9e41c6a3dd50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# create a new generator\n",
        "imagegen = ImageDataGenerator()\n",
        "# load train data\n",
        "train = imagegen.flow_from_directory(\"imagenette2/train/\", class_mode=\"categorical\", shuffle=False, batch_size=128, target_size=(224, 224))\n",
        "# load val data\n",
        "val = imagegen.flow_from_directory(\"imagenette2/val/\", class_mode=\"categorical\", shuffle=False, batch_size=128, target_size=(224, 224))\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 9469 images belonging to 10 classes.\n",
            "Found 3925 images belonging to 10 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pw7zUnE3TWzD"
      },
      "source": [
        "3. Building a Basic CNN model for Image Classification\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Letâ€™s build a basic CNN model for our Imagenette dataset (for the purpose of image classification):\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUR9ChHqTZgw",
        "outputId": "1745aecb-8597-471c-b28a-7399869ca79a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPool2D, Flatten, Dense, InputLayer, BatchNormalization, Dropout\n",
        "\n",
        "# build a sequential model\n",
        "model = Sequential()\n",
        "model.add(InputLayer(input_shape=(224, 224, 3)))\n",
        "\n",
        "# 1st conv block\n",
        "model.add(Conv2D(25, (5, 5), activation='relu', strides=(1, 1), padding='same'))\n",
        "model.add(MaxPool2D(pool_size=(2, 2), padding='same'))\n",
        "# 2nd conv block\n",
        "model.add(Conv2D(50, (5, 5), activation='relu', strides=(2, 2), padding='same'))\n",
        "model.add(MaxPool2D(pool_size=(2, 2), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "# 3rd conv block\n",
        "model.add(Conv2D(70, (3, 3), activation='relu', strides=(2, 2), padding='same'))\n",
        "model.add(MaxPool2D(pool_size=(2, 2), padding='valid'))\n",
        "model.add(BatchNormalization())\n",
        "# ANN block\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=100, activation='relu'))\n",
        "model.add(Dense(units=100, activation='relu'))\n",
        "model.add(Dropout(0.25))\n",
        "# output layer\n",
        "model.add(Dense(units=10, activation='softmax'))\n",
        "\n",
        "# compile model\n",
        "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
        "# fit on data for 30 epochs\n",
        "model.fit_generator(train, epochs=10, validation_data=val)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "74/74 [==============================] - 727s 10s/step - loss: 2.5413 - accuracy: 0.1450 - val_loss: 3.6511 - val_accuracy: 0.1057\n",
            "Epoch 2/10\n",
            "74/74 [==============================] - 724s 10s/step - loss: 2.1670 - accuracy: 0.2376 - val_loss: 2.0940 - val_accuracy: 0.2907\n",
            "Epoch 3/10\n",
            "74/74 [==============================] - 726s 10s/step - loss: 1.9786 - accuracy: 0.3208 - val_loss: 2.0564 - val_accuracy: 0.3050\n",
            "Epoch 4/10\n",
            "74/74 [==============================] - 726s 10s/step - loss: 1.8265 - accuracy: 0.3784 - val_loss: 2.0404 - val_accuracy: 0.2815\n",
            "Epoch 5/10\n",
            "74/74 [==============================] - 720s 10s/step - loss: 1.5450 - accuracy: 0.4819 - val_loss: 1.8828 - val_accuracy: 0.3880\n",
            "Epoch 6/10\n",
            "74/74 [==============================] - 723s 10s/step - loss: 1.2569 - accuracy: 0.5826 - val_loss: 2.3035 - val_accuracy: 0.3518\n",
            "Epoch 7/10\n",
            "74/74 [==============================] - 729s 10s/step - loss: 1.0785 - accuracy: 0.6395 - val_loss: 2.2653 - val_accuracy: 0.3307\n",
            "Epoch 8/10\n",
            "74/74 [==============================] - 730s 10s/step - loss: 0.8956 - accuracy: 0.7025 - val_loss: 2.1430 - val_accuracy: 0.4130\n",
            "Epoch 9/10\n",
            "74/74 [==============================] - 729s 10s/step - loss: 0.7584 - accuracy: 0.7447 - val_loss: 2.1170 - val_accuracy: 0.4339\n",
            "Epoch 10/10\n",
            "74/74 [==============================] - 726s 10s/step - loss: 0.5490 - accuracy: 0.8192 - val_loss: 2.0191 - val_accuracy: 0.4354\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fee02b4da20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mf6YbQzRTz0s"
      },
      "source": [
        "4. Using Transfer Learning (VGG16) to improve accuracy\n",
        "VGG16 is a CNN architecture that was the first runner-up in the 2014 ImageNet Challenge. Itâ€™s designed by the Visual Graphics Group at Oxford and has 16 layers in total, with 13 convolutional layers themselves. We will load the pre-trained weights of this model so that we can utilize the useful features this model has learned for our task.\n",
        "\n",
        "Downloading weights of VGG16\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAQli7lVn1yo",
        "outputId": "ced14b38-6cf1-40f3-8d67-ca01ffe55961",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        }
      },
      "source": [
        "from keras.applications import VGG16\n",
        "\n",
        "# include top should be False to remove the softmax layer\n",
        "pretrained_model = VGG16(include_top=False, weights='imagenet')\n",
        "pretrained_model.summary()\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 0s 0us/step\n",
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, None, None, 3)]   0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWwCzFyfoCeu"
      },
      "source": [
        "Generate features from VGG16\n",
        "Letâ€™s extract useful features that VGG16 already knows from our datasetâ€™s images:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0baiTQk_oDJN"
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "# extract train and val features\n",
        "vgg_features_train = pretrained_model.predict(train)\n",
        "vgg_features_val = pretrained_model.predict(val)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8quTYBOoJ3p"
      },
      "source": [
        "# OHE target column\n",
        "train_target = to_categorical(train.labels)\n",
        "val_target = to_categorical(val.labels)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-O_1Yvtok3G"
      },
      "source": [
        "Once the above features are ready, we can just use them to train a basic Fully Connected Neural Network in Keras:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCrJKTjSob3m"
      },
      "source": [
        "model2 = Sequential()\n",
        "model2.add(Flatten(input_shape=(7,7,512)))\n",
        "model2.add(Dense(100, activation='relu'))\n",
        "model2.add(Dropout(0.5))\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# compile the model\n",
        "model2.compile(optimizer='adam', metrics=['accuracy'], loss='categorical_crossentropy')\n",
        "\n",
        "model2.summary()\n",
        "\n",
        "# train model using features generated from VGG16 model\n",
        "model2.fit(vgg_features_train, train_target, epochs=50, batch_size=128, validation_data=(vgg_features_val, val_target))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}